# Spark DataFrame 의 `filter` 메소드

+ `filter()`는 SQL의 `WHERE`절과 비슷하게 DataFrame 내의 특정 **조건**을 만족하는 레코드를 **DataFrame**으로 반환.

+ `filter()`내의 조건 컬럼은 컬럼 속성으로 지정 가능. 
+ 조건문 자체는 SQL과 유사한 문자열로 지정할 수 있다.

+ `where()` 메소드는 `filter()`의 alias 함수이며, where의 직관적인 동일성을 간주하기 위해 생성
  - `filter() == where()`

+ 복합 조건 `and`, `or`는 각각 `&`, `|`를 사용.

+ 어떻게 보면 sql의 작성순서를 비슷하게 따라간다고 볼수도 있다.

  + from -> where -> select

  ```python
  data_sdf.filter(upper(data_sdf["Gender"]).like("%A%")).select("Name" , "Year").show()
  ```

  결과

  ```python
  +----+----+
  |Name|Year|
  +----+----+
  |민석|2011|
  |민호|2016|
  |성현|2015|
  |현주|2015|
  |상기|2011|
  +----+----+
  ```

​		(from : data_sdf,  where절 : filter~,  select절 : select~  )



## filter 사용방법

```python
# 잘못된 방법
data_sdf.filter('Name' == 'suyeee') # Error!!!

# SQL의 where 절 처럼 사용
data_sdf.filter(" Name = 'suyeee' ").show() # SELECT * FROM data_sdf WHERE Name = '민호';

# 컬럼기반 선택 활용하기
data_sdf.filter(data_sdf["Name"] == "suyeee").show()
data_sdf.filter(col("Name") == "suyeee").show() # 이 방법이 비교적 많이 사용된다.
data_sdf.filter(data_sdf.Name == "suyeee").show()
```





# **Spark DataFrame의** `orderBy()`



## Pandas 에서는

```python
# Pandas DataFrame에서는 - sort_values
titanic_pdf_sorted_01 = titanic_pdf.sort_values(by=["Name"], ascending=True)

# 정렬의 기준을 여러 개 놓기
titanic_pdf_sorted_02 = titanic_pdf.sort_values(by=["Pclass", "Name"], ascending=False)

# 정렬의 형식도 여러 개 지정이 가능!
titanic_pdf_sorted_03 = titanic_pdf.sort_values(by=["Pclass", "Name"], ascending=[True, False])

titanic_pdf_sorted_03
```

|      | PassengerId | Survived | Pclass |                                   Name |    Sex |  Age | SibSp | Parch |    Ticket |     Fare | Cabin | Embarked |
| ---: | ----------: | -------: | -----: | -------------------------------------: | -----: | ---: | ----: | ----: | --------: | -------: | ----: | -------: |
|  325 |         326 |        1 |      1 |               Young, Miss. Marie Grice | female | 36.0 |     0 |     0 |  PC 17760 | 135.6333 |   C32 |        C |
|  555 |         556 |        0 |      1 |                     Wright, Mr. George |   male | 62.0 |     0 |     0 |    113807 |  26.5500 |   NaN |        S |
|   55 |          56 |        1 |      1 |                      Woolner, Mr. Hugh |   male |  NaN |     0 |     0 |     19947 |  35.5000 |   C52 |        S |
|  351 |         352 |        0 |      1 | Williams-Lambert, Mr. Fletcher Fellows |   male |  NaN |     0 |     0 |    113510 |  35.0000 |  C128 |        S |
|  155 |         156 |        0 |      1 |            Williams, Mr. Charles Duane |   male | 51.0 |     0 |     1 |  PC 17597 |  61.3792 |   NaN |        C |
|  ... |         ... |      ... |    ... |                                    ... |    ... |  ... |   ... |   ... |       ... |      ... |   ... |      ... |
|  401 |         402 |        0 |      3 |                        Adams, Mr. John |   male | 26.0 |     0 |     0 |    341826 |   8.0500 |   NaN |        S |
|  365 |         366 |        0 |      3 |         Adahl, Mr. Mauritz Nils Martin |   male | 30.0 |     0 |     0 |    C 7076 |   7.2500 |   NaN |        S |
|  279 |         280 |        1 |      3 |       Abbott, Mrs. Stanton (Rosa Hunt) | female | 35.0 |     1 |     1 | C.A. 2673 |  20.2500 |   NaN |        S |
|  746 |         747 |        0 |      3 |            Abbott, Mr. Rossmore Edward |   male | 16.0 |     1 |     1 | C.A. 2673 |  20.2500 |   NaN |        S |
|  845 |         846 |        0 |      3 |                    Abbing, Mr. Anthony |   male | 42.0 |     0 |     0 | C.A. 5547 |   7.5500 |   NaN |        S |

891 rows × 12 columns



## Spark DataFrame에서는

```python
from pyspark.sql.functions import col

print("orderBy에 컬럼명을 문자열로 지정하고 내림 차순 정렬")
titanic_sdf.orderBy("Name", ascending=False).show()

# 직접 컬럼을 선택해서 내림 차순 정렬
titanic_sdf.orderBy(titanic_sdf["Name"], ascending=True).show()
titanic_sdf.orderBy(titanic_sdf.Name, ascending=True).show()
titanic_sdf.orderBy(col("Name"), ascending=True).show()
```

결과

```python
orderBy에 컬럼명을 문자열로 지정하고 내림 차순 정렬
+-----------+--------+------+--------------------+------+----+-----+-----+----------------+--------+-----+--------+
|PassengerId|Survived|Pclass|                Name|   Sex| Age|SibSp|Parch|          Ticket|    Fare|Cabin|Embarked|
+-----------+--------+------+--------------------+------+----+-----+-----+----------------+--------+-----+--------+
|        869|       0|     3|van Melkebeke, Mr...|  male|null|    0|    0|          345777|     9.5| null|       S|
|        154|       0|     3|van Billiard, Mr....|  male|40.5|    0|    2|        A/5. 851|    14.5| null|       S|
|        362|       0|     2|del Carlo, Mr. Se...|  male|29.0|    1|    0|   SC/PARIS 2167| 27.7208| null|       C|
|        283|       0|     3|de Pelsmaeker, Mr...|  male|16.0|    0|    0|          345778|     9.5| null|       S|
|        287|       1|     3|de Mulder, Mr. Th...|  male|30.0|    0|    0|          345774|     9.5| null|       S|
|        560|       1|     3|de Messemaeker, M...|female|36.0|    1|    0|          345572|    17.4| null|       S|
|        423|       0|     3|  Zimmerman, Mr. Leo|  male|29.0|    0|    0|          315082|   7.875| null|       S|
|        241|       0|     3|Zabour, Miss. Tha...|female|null|    1|    0|            2665| 14.4542| null|       C|
|        112|       0|     3|Zabour, Miss. Hileni|female|14.5|    1|    0|            2665| 14.4542| null|       C|
...

```



### **여러개의 컬럼 지정하기**

```python
titanic_sdf.orderBy("Pclass", "Name", ascending=False).show()
titanic_sdf.orderBy(["Pclass", "Name"], ascending=False).show()
titanic_sdf.orderBy(col("Pclass"), col("Name"), ascending=False).show()
```

결과

```python
+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+
|PassengerId|Survived|Pclass|                Name|   Sex| Age|SibSp|Parch|          Ticket|   Fare|Cabin|Embarked|
+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+
|        869|       0|     3|van Melkebeke, Mr...|  male|null|    0|    0|          345777|    9.5| null|       S|
|        154|       0|     3|van Billiard, Mr....|  male|40.5|    0|    2|        A/5. 851|   14.5| null|       S|
|        283|       0|     3|de Pelsmaeker, Mr...|  male|16.0|    0|    0|          345778|    9.5| null|       S|
|        287|       1|     3|de Mulder, Mr. Th...|  male|30.0|    0|    0|          345774|    9.5| null|       S|
|        560|       1|     3|de Messemaeker, M...|female|36.0|    1|    0|          345572|   17.4| null|       S|
|        423|       0|     3|  Zimmerman, Mr. Leo|  male|29.0|    0|    0|          315082|  7.875| null|       S|
|        241|       0|     3|Zabour, Miss. Tha...|female|null|    1|    0|            2665|14.4542| null|       C|
|        112|       0|     3|Zabour, Miss. Hileni|female|14.5|    1|    0|            2665|14.4542| null|       C|

...

```



### **여러개의 컬럼 정렬 방향 다르게 지정하기**

```python
titanic_sdf.orderBy("Pclass", "Name", ascending=[True, False]).show()
titanic_sdf.orderBy(col("Pclass"), col("Name"), ascending=[True, False]).show()
titanic_sdf.orderBy(col("Pclass").asc(), col("Name").desc()).show()
```

결과

```python
+-----------+--------+------+--------------------+------+----+-----+-----+--------+--------+-----+--------+
|PassengerId|Survived|Pclass|                Name|   Sex| Age|SibSp|Parch|  Ticket|    Fare|Cabin|Embarked|
+-----------+--------+------+--------------------+------+----+-----+-----+--------+--------+-----+--------+
|        326|       1|     1|Young, Miss. Mari...|female|36.0|    0|    0|PC 17760|135.6333|  C32|       C|
|        556|       0|     1|  Wright, Mr. George|  male|62.0|    0|    0|  113807|   26.55| null|       S|
|         56|       1|     1|   Woolner, Mr. Hugh|  male|null|    0|    0|   19947|    35.5|  C52|       S|
|        352|       0|     1|Williams-Lambert,...|  male|null|    0|    0|  113510|    35.0| C128|       S|

...

```





### **추출 -> 정렬  VS  정렬 -> 추출**

```python
# 데이터를 추출 하고(Pclass, Name을 select)하고 정렬
# select Pclass, Name from titanic_sdf order by Pclass asc, Name desc
titanic_sdf.select(col("Pclass"), col("name")).orderBy(col("Pclass").asc(), col("name").desc()).show()

# 데이터를 정렬 하고 추출(Pclass, Name을 select)
# select Pclass, Name from ( select * from titanic_sdf order by Pclass asc, Name desc )
titanic_sdf.orderBy(col("Pclass").asc(), col("Name").desc()).select(col("Pclass"), col("Name")).show()
```

결과

```python
+------+--------------------+
|Pclass|                name|
+------+--------------------+
|     1|Young, Miss. Mari...|
|     1|  Wright, Mr. George|
|     1|   Woolner, Mr. Hugh|
|     1|Williams-Lambert,...|
|     1|Williams, Mr. Cha...|
|     1|Widener, Mr. Harr...|
|     1|Wick, Mrs. George...|
|     1|Wick, Miss. Mary ...|
|     1|White, Mr. Richar...|
...


```





# Spark DataFrame의 `aggregation` 메소드 적용

+ `pandas Dataframe`은 `DataFrame`객체에서 집계(aggregate)에 관련된 메소드가 여러 개가 있다.

  + `DataFrame.count()`, `DataFrame.max()`

  - `DataFrame`에 속한 모든 컬럼들에 대한 `aggregation`

- `spark DataFrame`은 `DataFrame`객체에서 집계(aggregate)에 관련된 메소드가 많이 없다. `count()` 메소드 정도...

  - aggregation을 적용하기 위해서는 `pyspark.sql.functions` 모듈의 함수들을 사용해야 한다.

- SQL에서 데이터의 개수를 확인 하려면 `count(*)`을 시용하면된다. `count`는 데이터의 개수를 세기 위해서 컬럼 지정이 필요하지 않다!!

  - `count`를 제외한 `max, sum, min` 같은 집계함수들은 <u>**반드시 컬럼 지정이 필요**</u>하다. select 절에서!! ........ex) `max("Age")`

  ```python
  titanic_sdf.max() # select max(*) from titanic_sdf.. -> Error SQL
  ```

  



# Spark DataFrame의 `groupBy()`

- `pandas DataFrame`의 `groupby(by='컬럼명')` 수행하면 `컬럼명` 레벨로 group by 된 `DataFrameGroupBy`객체가 반환되고, 그 다음에 aggregation 메소드 적용
- `spark DataFrame`도 `groupBy(by='컬럼명')` 수행하면 `컬럼명` 레벨로 group by 된 `GroupedData`객체가 반환되고, 그 다음에 aggregation 메소드 적용
- `pandas DataFrameGroupBy` 객체에 `agg()` 메소드를 이용해서 서로 다른 컬럼에서 서로 다른 `aggregation`수행 가능
- `spark GroupedData` 객체도 `agg()` 메소드를 이용하여 서로 다른 컬럼에 서로 다른 `aggregation` 수행 가능
  - <u>`spark의 groupBy`는 `pandas groupby()`의 특징과, SQL의 특징을 함께 갖는다.</u>

+ ***pandas*** 에서 `groupby`가 중요했던것 처럼 ***spark***에서도 `groupBy`가 중요하다.
+ 컬럼별로 집계를 따로 지정해줄수도있다.

```python
# agg 함수를 활용해서 각 컬럼에 대한 집계를 따로따로 수행
agg_format = {
    "Age": "max",
    "SibSp": "sum",
    "Fare": "mean"
}

titanic_pdf_groupby.agg(agg_format)
```

+ 주의할점

```python
# GroupedData에서 aggregation 메소드를 호출 할때는 오직 문자열 컬럼명만 가능. 컬럼형 인자는 오류 발생.
titanic_sdf.groupBy("Pclass").max(col("Age")).show() # Error!!
```



**Pclass별 Age의 최대값, 최소값, 합, 평균을 구하는데, 최대 나이가 70 초과인 데이터 조회**

```sql
# sql로 보면
select max(age) as max_age,
       min(age) as min_age,
       sum(age) as sum_age,
       avg(age) as avg_age
from titanic_sdf
group by Pclass
having max(age) > 70

"Spark GroupedData에는 having이 없다. - 그룹핑을 해서 값을 구하고 그 다음 filter를 적용"

select max_age, min_age, sum_age, avg_age
from
    (
        select max(age) as max_age,
               min(age) as min_age,
               sum(age) as sum_age,
               avg(age) as avg_age
        from titanic_sdf
        group by Pclass
    )
where max_age > 70
```



실습해보기

```python
# 쿼리로 할땐 원래는 groupBy에 대한 조건이니 having을 써야하는데 스파크에는 having이 없다!!!

# 내가 한 버전
from pyspark.sql.functions import max,sum,min,avg
titanic_sdf.filter("Age > 70 ").groupBy("Pclass").agg(max("Age"), min("Age"),sum("Age"),avg("Age")).show() 
 
# 강사님 버전    
titanic_sdf.groupBy("Pclass").agg(
    max("Age").alias("max_age"),
    min("Age").alias("min_age"),
    sum("Age").alias("sum_age"),
    avg("Age").alias("avg_age")
).filter(col("max_age") > 70).show()  
```

결과

```python
# 내가 한 버전
+------+--------+--------+--------+--------+
|Pclass|max(Age)|min(Age)|sum(Age)|avg(Age)|
+------+--------+--------+--------+--------+
|     1|    80.0|    71.0|   222.0|    74.0|
|     3|    74.0|    70.5|   144.5|   72.25|
+------+--------+--------+--------+--------+

# 강사님 버전
+------+-------+-------+-------+------------------+
|Pclass|max_age|min_age|sum_age|           avg_age|
+------+-------+-------+-------+------------------+
|     1|   80.0|   0.92|7111.42|38.233440860215055|
|     3|   74.0|   0.42|8924.92| 25.14061971830986|
+------+-------+-------+-------+------------------+
```

내가한거는 필터링을 먼저 해서 그런지 age가 70인 데이터로만 걸러져서 강사님이 하신거보다 `min,sum` 은 적게나오고 평균은 높게 나온것 같다. `filter`를 `where절` 처럼 인식하고 있어서 그런지 왠지 쿼리 작성 순서대로 from절 다음 where절이 와야될것같다. 그래서 from절에 대응되는 titanic_sdf 다음에 바로 filter를 사용했다.. Spark에는 having이 없으니 강사님이 하신 방법처럼 filter를 having 처럼 쓰면 될거같다.



## filter, aggregation, groupBy() 실습

[**08. Spark DataFrame 심화**](https://github.com/suyeee/todo_study/blob/0bbbad7fe36b267950a2de57aff009af7c5ba7e5/220516_s.assets/08.%20Spark%20DataFrame%20%20%EC%8B%AC%ED%99%94.ipynb)





# Spark Backend

![image-20220516130221920](220516.assets/image-20220516130221920.png)

![image-20220516130333630](220516.assets/image-20220516130333630.png)



+ 카탈리스트 -> 계획만 세워주는애
+ 카탈리스트가 알아서 다 계획을 세워준다.

+ 텅스텐 -> LOW 레벨에서 성능을 최대로 발휘할수있게 해주는애



![image-20220516130707632](220516.assets/image-20220516130707632.png)



![image-20220516130940851](220516.assets/image-20220516130940851.png)

+ 카탈리스트가 하는일 -> 논리적인 플랜을 물리적인 플랜으로 바꿔주는 역할



![image-20220516131354625](220516.assets/image-20220516131354625.png)

![image-20220516131404491](220516.assets/image-20220516131404491.png)

![image-20220516131419859](220516.assets/image-20220516131419859.png)

+ Join 먼저 실행하면 느려지니 필터링을 먼저하고 조인 해준다. -> 알아서 해줌.



![image-20220516131604212](220516.assets/image-20220516131604212.png)



![image-20220516131839621](220516.assets/image-20220516131839621.png)



+ 데이터의 양이 줄어들어야 좋으니 필터링을 먼저 진행한다
+ 실제 최적화가 되는 부분 -> table scan , filter 로 바꿔준다??? 



![image-20220516132104130](220516.assets/image-20220516132104130.png)

+ 아직 최적화는 안된 상태



최적화 대상을 분석하는 과정 -> Analyzw logical Plan

![image-20220516132349706](220516.assets/image-20220516132349706.png)



## 텅스텐

+ 이 프로세스를 **코드 제너레이션** 이라고 한다.
+ 어떤 클러스터가 자원이 남아도니 더 일을 시켜야겠다 -> 이걸 담당하는 애
+ 워커들이 어떻게 일을 해야할지, 더 빠르게 최적화되서 일을 하게할껀지를 얘기해주는애





# UDF

- udf를 안쓰면 마스터 노드가 일을 하게된다
  - 마스터 노드는 일을 하는 애가 아닌데
- ![image-20220516142829701](220516.assets/image-20220516142829701.png)
- 해결방법 : 워커를 만들어준다



![image-20220516143256449](220516.assets/image-20220516143256449.png)





+ 쿼리 연습 많이 하기!





# 과제

```markdown
# 1. 연령 별 count 세어주기
- `fakefriends.csv`
- age가 13세 이상 19세 미만인 사람들 조회
- 연령 별 count를 DataFrame API 사용해서 구해보기
- 연령 별 친구 수의 평균 구하기

# 2. wordcount
- `Book`
- pyspark.sql.functions의 explode 함수 활용
    - flatMap과 흡사한 기능
- DataFrame API를 활용해서 워드카운트 구현
- 단어 수가 많은 순대로(내림차순) 정렬

# 3. 최저 온도 구하기
- `1800.csv`
- 지역 별 최저 온도 구하기
- DataFrame API
```

