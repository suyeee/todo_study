## Spark DataFrame 의 filter 메소드

+ 스파크는 쿼리를 사용하거나 컬럼선택으로 어떤걸 쓸건지를 알려줬다

+ 스파크는 항상 컬럼 속성이 필요하다.

+ 스파크는 항상 명확하게 선택을 해줘야한다.

+ 어떻게 보면 sql의 작성순서를 비슷하게 따라간다고 볼수도 있다.

  + from -> where -> select

  ```python
  data_sdf.filter(upper(data_sdf["Gender"]).like("%A%")).select("Name" , "Year").show()
  ```

  결과

  ```python
  +----+----+
  |Name|Year|
  +----+----+
  |민석|2011|
  |민호|2016|
  |성현|2015|
  |현주|2015|
  |상기|2011|
  +----+----+
  ```

  



## Spark DataFrame의 aggregation 메소드 적용





## Spark DataFrame의 groupBy()

+ 판다스에서 groupby가 중요했던것 처럼
+ 스파크에서도 groupBy가 중요하다

```python
# 쿼리로 할땐 원래는 groupBy에 대한 조건이니 having을 써야하는데 스파크에는 해빙이 없다.!!!
from pyspark.sql.functions import max,sum,min,avg
titanic_sdf.filter("Age > 70 ").groupBy("Pclass").agg(max("Age"), min("Age"),sum("Age"),avg("Age")).show() # 내가 한 버전
 
titanic_sdf.groupBy("Pclass").agg(
    max("Age").alias("max_age"),
    min("Age").alias("min_age"),
    sum("Age").alias("sum_age"),
    avg("Age").alias("avg_age")
).filter(col("max_age") > 70).show()  # 강사님 버전
```

결과

```python
+------+--------+--------+--------+--------+
|Pclass|max(Age)|min(Age)|sum(Age)|avg(Age)|
+------+--------+--------+--------+--------+
|     1|    80.0|    71.0|   222.0|    74.0|
|     3|    74.0|    70.5|   144.5|   72.25|
+------+--------+--------+--------+--------+

+------+-------+-------+-------+------------------+
|Pclass|max_age|min_age|sum_age|           avg_age|
+------+-------+-------+-------+------------------+
|     1|   80.0|   0.92|7111.42|38.233440860215055|
|     3|   74.0|   0.42|8924.92| 25.14061971830986|
+------+-------+-------+-------+------------------+
```



# Spark Backend

![image-20220516130221920](220516.assets/image-20220516130221920.png)

![image-20220516130333630](220516.assets/image-20220516130333630.png)



+ 카탈리스트 -> 계획만 세워주는애
+ 카탈리스트가 알아서 다 계획을 세워준다.

+ 텅스텐 -> LOW 레벨에서 성능을 최대로 발휘할수있게 해주는애



![image-20220516130707632](220516.assets/image-20220516130707632.png)



![image-20220516130940851](220516.assets/image-20220516130940851.png)

+ 카탈리스트가 하는일 -> 논리적인 플랜을 물리적인 플랜으로 바꿔주는 역할



![image-20220516131354625](220516.assets/image-20220516131354625.png)

![image-20220516131404491](220516.assets/image-20220516131404491.png)

![image-20220516131419859](220516.assets/image-20220516131419859.png)

+ Join 먼저 실행하면 느려지니 필터링을 먼저하고 조인 해준다. -> 알아서 해줌.



![image-20220516131604212](220516.assets/image-20220516131604212.png)



![image-20220516131839621](220516.assets/image-20220516131839621.png)



+ 데이터의 양이 줄어들어야 좋으니 필터링을 먼저 진행한다
+ 실제 최적화가 되는 부분 -> table scan , filter 로 바꿔준다??? 



![image-20220516132104130](220516.assets/image-20220516132104130.png)

+ 아직 최적화는 안된 상태



최적화 대상을 분석하는 과정 -> Analyzw logical Plan

![image-20220516132349706](220516.assets/image-20220516132349706.png)



## 텅스텐

+ 이 프로세스를 **코드 제너레이션** 이라고 한다.
+ 어떤 클러스터가 자원이 남아도니 더 일을 시켜야겠다 -> 이걸 담당하는 애
+ 워커들이 어떻게 일을 해야할지, 더 빠르게 최적화되서 일을 하게할껀지를 얘기해주는애





# UDF

- udf를 안쓰면 마스터 노드가 일을 하게된다
  - 마스터 노드는 일을 하는 애가 아닌데
- ![image-20220516142829701](220516.assets/image-20220516142829701.png)
- 해결방법 : 워커를 만들어준다



![image-20220516143256449](220516.assets/image-20220516143256449.png)





+ 쿼리 연습 많이 하기!



두번째 left join 할때 on t.DOLocationID = dz.LocationID 이거 아닌가요? .. t.PUL 로 되어있어요





# 과제

```markdown
# 1. 연령 별 count 세어주기
- `fakefriends.csv`
- age가 13세 이상 19세 미만인 사람들 조회
- 연령 별 count를 DataFrame API 사용해서 구해보기
- 연령 별 친구 수의 평균 구하기

# 2. wordcount
- `Book`
- pyspark.sql.functions의 explode 함수 활용
    - flatMap과 흡사한 기능
- DataFrame API를 활용해서 워드카운트 구현
- 단어 수가 많은 순대로(내림차순) 정렬

# 3. 최저 온도 구하기
- `1800.csv`
- 지역 별 최저 온도 구하기
- DataFrame API
```

