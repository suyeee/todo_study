# Spark DataFrame 의 filter 메소드

+ `filter()`는 SQL의 `WHERE`절과 비슷하게 DataFrame 내의 특정 **조건**을 만족하는 레코드를 **DataFrame**으로 반환.

+ `filter()`내의 조건 컬럼은 컬럼 속성으로 지정 가능. 
+ 조건문 자체는 SQL과 유사한 문자열로 지정할 수 있다.

+ `where()` 메소드는 `filter()`의 alias 함수이며, where의 직관적인 동일성을 간주하기 위해 생성
  - `filter() == where()`

+ 복합 조건 `and`, `or`는 각각 `&`, `|`를 사용.

+ 어떻게 보면 sql의 작성순서를 비슷하게 따라간다고 볼수도 있다.

  + from -> where -> select

  ```python
  data_sdf.filter(upper(data_sdf["Gender"]).like("%A%")).select("Name" , "Year").show()
  ```

  




## filter 사용방법

```python
# 잘못된 방법
data_sdf.filter('Name' == 'suyeee') # Error!!!

# SQL의 where 절 처럼 사용
data_sdf.filter(" Name = 'suyeee' ").show() # SELECT * FROM data_sdf WHERE Name = '민호';

# 컬럼기반 선택 활용하기
data_sdf.filter(data_sdf["Name"] == "suyeee").show()
data_sdf.filter(col("Name") == "suyeee").show() # 이 방법이 비교적 많이 사용된다.
data_sdf.filter(data_sdf.Name == "suyeee").show()
```



# Spark DataFrame의 aggregation 메소드 적용







## Spark DataFrame의 groupBy()

+ 판다스에서 groupby가 중요했던것 처럼
+ 스파크에서도 groupBy가 중요하다

```python
# 쿼리로 할땐 원래는 groupBy에 대한 조건이니 having을 써야하는데 스파크에는 해빙이 없다.!!!
from pyspark.sql.functions import max,sum,min,avg
titanic_sdf.filter("Age > 70 ").groupBy("Pclass").agg(max("Age"), min("Age"),sum("Age"),avg("Age")).show() # 내가 한 버전
 
titanic_sdf.groupBy("Pclass").agg(
    max("Age").alias("max_age"),
    min("Age").alias("min_age"),
    sum("Age").alias("sum_age"),
    avg("Age").alias("avg_age")
).filter(col("max_age") > 70).show()  # 강사님 버전
```

결과

```python
+------+--------+--------+--------+--------+
|Pclass|max(Age)|min(Age)|sum(Age)|avg(Age)|
+------+--------+--------+--------+--------+
|     1|    80.0|    71.0|   222.0|    74.0|
|     3|    74.0|    70.5|   144.5|   72.25|
+------+--------+--------+--------+--------+

+------+-------+-------+-------+------------------+
|Pclass|max_age|min_age|sum_age|           avg_age|
+------+-------+-------+-------+------------------+
|     1|   80.0|   0.92|7111.42|38.233440860215055|
|     3|   74.0|   0.42|8924.92| 25.14061971830986|
+------+-------+-------+-------+------------------+
```





## filter, aggregation, groupBy() 실습

[**08. Spark DataFrame 심화**](https://github.com/suyeee/todo_study/blob/0bbbad7fe36b267950a2de57aff009af7c5ba7e5/220516_s.assets/08.%20Spark%20DataFrame%20%20%EC%8B%AC%ED%99%94.ipynb)





# Spark Backend

![image-20220516130221920](220516.assets/image-20220516130221920.png)

![image-20220516130333630](220516.assets/image-20220516130333630.png)



+ 카탈리스트 -> 계획만 세워주는애
+ 카탈리스트가 알아서 다 계획을 세워준다.

+ 텅스텐 -> LOW 레벨에서 성능을 최대로 발휘할수있게 해주는애



![image-20220516130707632](220516.assets/image-20220516130707632.png)



![image-20220516130940851](220516.assets/image-20220516130940851.png)

+ 카탈리스트가 하는일 -> 논리적인 플랜을 물리적인 플랜으로 바꿔주는 역할



![image-20220516131354625](220516.assets/image-20220516131354625.png)

![image-20220516131404491](220516.assets/image-20220516131404491.png)

![image-20220516131419859](220516.assets/image-20220516131419859.png)

+ Join 먼저 실행하면 느려지니 필터링을 먼저하고 조인 해준다. -> 알아서 해줌.



![image-20220516131604212](220516.assets/image-20220516131604212.png)



![image-20220516131839621](220516.assets/image-20220516131839621.png)



+ 데이터의 양이 줄어들어야 좋으니 필터링을 먼저 진행한다
+ 실제 최적화가 되는 부분 -> table scan , filter 로 바꿔준다??? 



![image-20220516132104130](220516.assets/image-20220516132104130.png)

+ 아직 최적화는 안된 상태



최적화 대상을 분석하는 과정 -> Analyzw logical Plan

![image-20220516132349706](220516.assets/image-20220516132349706.png)



## 텅스텐

+ 이 프로세스를 **코드 제너레이션** 이라고 한다.
+ 어떤 클러스터가 자원이 남아도니 더 일을 시켜야겠다 -> 이걸 담당하는 애
+ 워커들이 어떻게 일을 해야할지, 더 빠르게 최적화되서 일을 하게할껀지를 얘기해주는애





# UDF

- udf를 안쓰면 마스터 노드가 일을 하게된다
  - 마스터 노드는 일을 하는 애가 아닌데
- ![image-20220516142829701](220516.assets/image-20220516142829701.png)
- 해결방법 : 워커를 만들어준다



![image-20220516143256449](220516.assets/image-20220516143256449.png)





+ 쿼리 연습 많이 하기!





# 과제

```markdown
# 1. 연령 별 count 세어주기
- `fakefriends.csv`
- age가 13세 이상 19세 미만인 사람들 조회
- 연령 별 count를 DataFrame API 사용해서 구해보기
- 연령 별 친구 수의 평균 구하기

# 2. wordcount
- `Book`
- pyspark.sql.functions의 explode 함수 활용
    - flatMap과 흡사한 기능
- DataFrame API를 활용해서 워드카운트 구현
- 단어 수가 많은 순대로(내림차순) 정렬

# 3. 최저 온도 구하기
- `1800.csv`
- 지역 별 최저 온도 구하기
- DataFrame API
```

