# VSCode 로 ssh 접속-> 주피터노트북 접속





접속 잘 된다.



# Airflow



+ Dag를 여러형식으로 관리를 해야될때-> 에어 플로우 사용





+ CRON





+ 순환되는 구조는 DAG로 볼수없다.
+ 작업들이 방향성이 있는 비순환구조이다?  ->  `DAG` 





+ Operator 
  + 실제 코딩을 하는 대상
  + 파이썬으로 구성





1. 실행
   + 웹서버 -> 스케쥴러
     + UI 관련 작업??
   + 스케줄러 -> 익스큐터
     + CLI 에서 작업??

2. 실행 정보 저장

3. 결과저장

+ 작업이 성공했는지 실패했다면 왜 실패했는지 등등 로그 기록들을 저장

+ DAG 는 익스큐터의 큐 때문에 되는것?







## 동작방식





## DAG 생성과 실행

> 프로그래밍 할때 신경써야할것들 위주로 얘기할예정



# VSCode 로 들어와서

가상환경 만들어주고

```powershell
(base) lab26@ip-000-00-00-0:~$ conda create -n lab26_env python=3.7
```



새롭게 만든 가상환경으로 접속

```powershell
(base) lab26@ip-000-00-00-0:~$ source activate lab26_env
```

```powershell
(lab26_env) lab26@ip-000-00-00-0:~$ 
```



에어플로우 설치

```powershell
(lab26_env) lab26@ip-000-00-00-0:~$ pip3 install apache-airflow
```



에어플로우로 들어간후 ls 로 로그? 확인

```powershell
(lab26_env) lab26@ip-000-00-00-0:~$ cd airflow
(lab26_env) lab26@ip-000-00-00-0:~/airflow$ ls
airflow.cfg  logs  webserver_config.py
```



DAG 생성

```powershell
(lab26_env) lab26@ip-000-00-00-0:~/airflow$ mkdir dags
(lab26_env) lab26@ip-000-00-00-0:~/airflow$ ls
airflow.cfg  dags  logs  webserver_config.py
```



에어플로우 초기화 해주기

```powershell
(lab26_env) lab26@ip-000-00-00-0:~/airflow$ airflow db init
(lab26_env) lab26@ip-000-00-00-0:~/airflow$ ls
airflow.cfg  airflow.db  dags  logs  webserver_config.py
```



에어플로우 웹서버 접속

```powershell
(lab26_env) lab26@ip-000-00-00-0:~/airflow$ airflow webserver -p 8932
```







`+` 버튼 클릭후 다른 터미널 열어서 스케줄러 동작시키기

```powershell
lab26@ip-000-00-00-0:~$ source activate lab26_env
```

```powershell
(lab26_env) lab26@ip-000-00-00-0:~$ airflow scheduler
```

결과

```powershell
  ____________       _____________
 ____    |__( )_________  __/__  /________      __
____  /| |_  /__  ___/_  /_ __  /_  __ \_ | /| / /
___  ___ |  / _  /   _  __/ _  / / /_/ /_ |/ |/ /
 _/_/  |_/_/  /_/    /_/    /_/  \____/____/|__/
[2022-05-18 11:51:45,710] {scheduler_job.py:693} INFO - Starting the scheduler
[2022-05-18 11:51:45 +0900] [1426] [INFO] Starting gunicorn 20.1.0
[2022-05-18 11:51:45,710] {scheduler_job.py:698} INFO - Processing each file at most -1 times
[2022-05-18 11:51:45 +0900] [1426] [INFO] Listening at: http://0.0.0.0:8793 (1426)
[2022-05-18 11:51:45 +0900] [1426] [INFO] Using worker: sync
[2022-05-18 11:51:45,713] {executor_loader.py:106} INFO - Loaded executor: SequentialExecutor
[2022-05-18 11:51:45,715] {manager.py:156} INFO - Launched DagFileProcessorManager with pid: 1428
[2022-05-18 11:51:45 +0900] [1427] [INFO] Booting worker with pid: 1427
[2022-05-18 11:51:45,717] {scheduler_job.py:1218} INFO - Resetting orphaned tasks for active dag runs
[2022-05-18 11:51:45,720] {settings.py:55} INFO - Configured default timezone Timezone('UTC')
[2022-05-18 11:51:45,731] {manager.py:402} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2) when using sqlite. So we set parallelism to 1.
[2022-05-18 11:51:45 +0900] [1430] [INFO] Booting worker with pid: 1430
```







스케쥴러 포트 바꾸기

```powershell
(lab26_env) lab26@ip-000-00-00-0:~$ export AIRFLOW__LOGGING__WORKER_LOG_SERVER_PORT=8726
```

```powershell
(lab26_env) lab26@ip-000-00-00-0:~$ airflow scheduler
```

```
airflow users create -u admin -p admin -f suyeon -l hwang -r Admin -e admin@admin.com
```



유저 계정 만들기

```powershell
lab26@ip-000-00-00-0:~$ source activate lab26_env
(lab26_env) lab26@ip-000-00-00-0:~$ cd airflow
(lab26_env) lab26@ip-000-00-00-0:~/airflow$ airflow users create -u admin -p admin -f suyeee -l h -r Admin -e admin@admin.com
```







```powershell
(lab26_env) lab26@ip-000-00-00-0:~/airflow$ airflow -h
usage: airflow [-h] GROUP_OR_COMMAND ...

positional arguments:
  GROUP_OR_COMMAND

    Groups:
      celery         Celery components
      config         View configuration
      connections    Manage connections
      dags           Manage DAGs
      db             Database operations
      jobs           Manage jobs
      kubernetes     Tools to help run the KubernetesExecutor
      pools          Manage pools
      providers      Display providers
      roles          Manage roles
      tasks          Manage tasks
      users          Manage users
      variables      Manage variables

    Commands:
      cheat-sheet    Display cheat sheet
      dag-processor  Start a standalone Dag Processor instance
      info           Show information about current Airflow and environment
      kerberos       Start a kerberos ticket renewer
      plugins        Dump information about loaded plugins
      rotate-fernet-key
                     Rotate encrypted connection credentials and variables
      scheduler      Start a scheduler instance
      standalone     Run an all-in-one copy of Airflow
      sync-perm      Update permissions for existing roles and optionally DAGs
      triggerer      Start a triggerer instance
      version        Show the version
      webserver      Start a Airflow webserver instance

optional arguments:
  -h, --help         show this help message and exit
```

+ scheduler 와 webserver 만 사용 예정



dags 확인해보기

```powershell
(lab26_env) lab26@ip-000-00-00-0:~/airflow$ airflow dags list
dag_id                                   | filepath                                                                                     | owner   | paused
=========================================+==============================================================================================+=========+=======
example_bash_operator                    | /home/ubuntu/anaconda3/envs/lab26_env/lib/python3.7/site-packages/airflow/example_dags/examp | airflow | True  
                                         | le_bash_operator.py                                                                          |         |       
example_branch_datetime_operator_2       | /home/ubuntu/anaconda3/envs/lab26_env/lib/python3.7/site-packages/airflow/example_dags/examp | airflow | True  
                                         | le_branch_datetime_operator.py                                                               |         |       
example_branch_dop_operator_v3           | /home/ubuntu/anaconda3/envs/lab26_env/lib/python3.7/site-packages/airflow/example_dags/examp | airflow | True  
                                         | le_branch_python_dop_operator_3.py                                                           |         |       
example_branch_labels                    | /home/ubuntu/anaconda3/envs/lab26_env/lib/python3.7/site-packages/airflow/example_dags/examp | airflow | True  
                                         | le_branch_labels.py                                                                          |         |       
example_branch_operator                  | /home/ubuntu/anaconda3/envs/lab26_env/lib/python3.7/site-packages/airflow/example_dags/examp | airflow | True  
                                         | le_branch_operator.py                                                                        |         |       
example_branch_python_operator_decorator | /home/ubuntu/anaconda3/envs/lab26_env/lib/python3.7/site-packages/airflow/example_dags/examp | airflow | True  
                                         | le_branch_operator_decorator.py                                                              |         |       
example_complex                          | /home/ubuntu/anaconda3/envs/lab26_env/lib/python3.7/site-packages/airflow/example_dags/examp | airflow | True  
                                         | le_complex.py                                                                                |         |       
example_dag_decorator                    | /home/ubuntu/anaconda3/envs/lab26_env/lib/python3.7/site-packages/airflow/example_dags/examp | airflow | True  
                                         | le_dag_decorator.py                                                                          |         |       
example_external_task_marker_child       | /home/ubuntu/anaconda3/envs/lab26_env/lib/python3.7/site-packages/airflow/example_dags/examp | airflow | True  
                                         | le_external_task_marker_dag.py                                                               |         |       
example_external_task_marker_parent      | /home/ubuntu/anaconda3/envs/lab26_env/lib/python3.7/site-packages/airflow/example_dags/examp | airflow | True  
                                         | le_external_task_marker_dag.py                                                               |         |       
example_nested_branch_dag                | /home/ubuntu/anaconda3/envs/lab26_env/lib/python3.7/site-packages/airflow/example_dags/examp | airflow | True  
                                         | le_nested_branch_dag.py                                                                      |         |       
example_passing_params_via_test_command  | /home/ubuntu/anaconda3/envs/lab26_env/lib/python3.7/site-packages/airflow/example_dags/examp | airflow | True  
                                         | le_passing_params_via_test_command.py                                                        |         |       
example_python_operator                  | /home/ubuntu/anaconda3/envs/lab26_env/lib/python3.7/site-packages/airflow/example_dags/examp | airflow | True  
                                         | le_python_operator.py                                                                        |         |       
example_short_circuit_operator           | /home/ubuntu/anaconda3/envs/lab26_env/lib/python3.7/site-packages/airflow/example_dags/examp | airflow | True  
                                         | le_short_circuit_operator.py                                                                 |         |       
example_skip_dag                         | /home/ubuntu/anaconda3/envs/lab26_env/lib/python3.7/site-packages/airflow/example_dags/examp | airflow | True  
                                         | le_skip_dag.py                                                                               |         |       
example_sla_dag                          | /home/ubuntu/anaconda3/envs/lab26_env/lib/python3.7/site-packages/airflow/example_dags/examp | airflow | True  
                                         | le_sla_dag.py                                                                                |         |       
example_subdag_operator                  | /home/ubuntu/anaconda3/envs/lab26_env/lib/python3.7/site-packages/airflow/example_dags/examp | airflow | True  
                                         | le_subdag_operator.py                                                                        |         |       
example_subdag_operator.section-1        | /home/ubuntu/anaconda3/envs/lab26_env/lib/python3.7/site-packages/airflow/example_dags/examp | airflow | True  
                                         | le_subdag_operator.py                                                                        |         |       
example_subdag_operator.section-2        | /home/ubuntu/anaconda3/envs/lab26_env/lib/python3.7/site-packages/airflow/example_dags/examp | airflow | True  
                                         | le_subdag_operator.py                                                                        |         |       
example_task_group                       | /home/ubuntu/anaconda3/envs/lab26_env/lib/python3.7/site-packages/airflow/example_dags/examp | airflow | True  
                                         | le_task_group.py                                                                             |         |       
example_task_group_decorator             | /home/ubuntu/anaconda3/envs/lab26_env/lib/python3.7/site-packages/airflow/example_dags/examp | airflow | True  
                                         | le_task_group_decorator.py                                                                   |         |       
example_time_delta_sensor_async          | /home/ubuntu/anaconda3/envs/lab26_env/lib/python3.7/site-packages/airflow/example_dags/examp | airflow | True  
                                         | le_time_delta_sensor_async.py                                                                |         |       
example_trigger_controller_dag           | /home/ubuntu/anaconda3/envs/lab26_env/lib/python3.7/site-packages/airflow/example_dags/examp | airflow | True  
                                         | le_trigger_controller_dag.py                                                                 |         |       
example_trigger_target_dag               | /home/ubuntu/anaconda3/envs/lab26_env/lib/python3.7/site-packages/airflow/example_dags/examp | airflow | True  
                                         | le_trigger_target_dag.py                                                                     |         |       
example_weekday_branch_operator          | /home/ubuntu/anaconda3/envs/lab26_env/lib/python3.7/site-packages/airflow/example_dags/examp | airflow | True  
                                         | le_branch_day_of_week_operator.py                                                            |         |       
example_xcom                             | /home/ubuntu/anaconda3/envs/lab26_env/lib/python3.7/site-packages/airflow/example_dags/examp | airflow | True  
                                         | le_xcom.py                                                                                   |         |       
example_xcom_args                        | /home/ubuntu/anaconda3/envs/lab26_env/lib/python3.7/site-packages/airflow/example_dags/examp | airflow | True  
                                         | le_xcomargs.py                                                                               |         |       
example_xcom_args_with_operators         | /home/ubuntu/anaconda3/envs/lab26_env/lib/python3.7/site-packages/airflow/example_dags/examp | airflow | True  
                                         | le_xcomargs.py                                                                               |         |       
latest_only                              | /home/ubuntu/anaconda3/envs/lab26_env/lib/python3.7/site-packages/airflow/example_dags/examp | airflow | True  
                                         | le_latest_only.py                                                                            |         |       
latest_only_with_trigger                 | /home/ubuntu/anaconda3/envs/lab26_env/lib/python3.7/site-packages/airflow/example_dags/examp | airflow | True  
                                         | le_latest_only_with_trigger.py                                                               |         |       
tutorial                                 | /home/ubuntu/anaconda3/envs/lab26_env/lib/python3.7/site-packages/airflow/example_dags/tutor | airflow | True  
                                         | ial.py                                                                                       |         |       
tutorial_etl_dag                         | /home/ubuntu/anaconda3/envs/lab26_env/lib/python3.7/site-packages/airflow/example_dags/tutor | airflow | True  
                                         | ial_etl_dag.py                                                                               |         |       
tutorial_taskflow_api_etl                | /home/ubuntu/anaconda3/envs/lab26_env/lib/python3.7/site-packages/airflow/example_dags/tutor | airflow | True  
                                         | ial_taskflow_api_etl.py                                                                      |         |       
```



Task 들 확인하기 (Task들 중 example_xcom만 확인해보기)

```powershell
(lab26_env) lab26@ip-000-00-00-0:~/airflow$ airflow tasks list example_xcom
bash_pull
bash_push
pull_value_from_bash_push
puller
push
push_by_returning
```





```powershell
(lab26_env) lab26@ip-000-00-00-0:~/airflow$ airflow dags trigger -e 2022-01-01 example_trigger_target_dag
[2022-05-18 13:36:23,670] {__init__.py:40} INFO - Loaded API auth backend: airflow.api.auth.backend.session
Created <DagRun example_trigger_target_dag @ 2022-01-01T00:00:00+00:00: manual__2022-01-01T00:00:00+00:00, externally triggered: True>
```



큐에 대치된 상태



DAG 폴더 열고서 웹서버랑 스케쥴러 에러 뜨는거 해결

```powershell
(lab26_env) lab26@ip-000-00-00-0:~/airflow/dags$ netstat -ntlp | grep :8726
(Not all processes could be identified, non-owned process info
 will not be shown, you would have to be root to see it all.)
tcp        0      0 0.0.0.0:8726            0.0.0.0:*               LISTEN      7107/gunicorn: work 
(lab26_env) lab26@ip-000-00-00-0:~/airflow/dags$ kill -9 7107
(lab26_env) lab26@ip-000-00-00-0:~/airflow/dags$ airflow scheduler
```



DAG 폴더에 새파일을 만들어준다

새파일명 : `kakao-pipeline.py`



+ 커넥션 만드는법

에어플로우 웹서버에서 어드민 -> 커넥션 페이지 들어가기







세이브 눌러서 저장





다시 DAG 로 들어가서 



트리거 실행해주면 잘 된다.



Sqlite 잘 실행되는지 확인

```powershell
(lab26_env) lab26@ip-000-00-00-0:~/airflow$ sqlite3 airflow.db
SQLite version 3.38.3 2022-04-27 12:03:15
Enter ".help" for usage hints.
sqlite> 
```



테이블 확인하기

```powershell
sqlite> .table
```

결과

```powershell
ab_permission                  job                          
ab_permission_view             kakao_search_result          
ab_permission_view_role        log                          
ab_register_user               log_template                 
ab_role                        rendered_task_instance_fields
ab_user                        sensor_instance              
ab_user_role                   serialized_dag               
ab_view_menu                   session                      
alembic_version                sla_miss                     
callback_request               slot_pool                    
connection                     task_fail                    
dag                            task_instance                
dag_code                       task_map                     
dag_pickle                     task_reschedule              
dag_run                        trigger                      
dag_tag                        variable                     
import_error                   xcom   
```



센서 api







터미널 새로 한개 더 열어서 Task 확인

```powershell
lab26@ip-000-00-00-0:~/airflow/dags$ source /home/ubuntu/anaconda3/bin/activate
(base) lab26@ip-000-00-00-0:~/airflow/dags$ conda activate lab26_env
(lab26_env) lab26@ip-000-00-00-0:~/airflow/dags$ cd ..
(lab26_env) lab26@ip-000-00-00-0:~/airflow$ airflow tasks test kakao-pipeline is_api_available 2022-01-01
```



 중간 전체코드

```python
from datetime import datetime
from airflow import DAG 

# Operator Import

# DB Operator - DB 관련 Operator
from airflow.providers.sqlite.operators.sqlite import SqliteOperator

# HTTP Sensor Operator - 웹 사이트에 HTTP 통신을 활용해 접근할수있는지 감지
from airflow.providers.http.sensors.http import HttpSensor

# 시작시간 반드시 필요
default_args = {
    "start_date" : datetime(2022, 1, 1) # 2022년 1월 1일 부터 시작.
}

KAKAO_API_KEY = "000000000000000000000" # https://developers.kakao.com/ 에서 REST API 키 확인

# DAG 환경을 구성하기위한 Task 정의 context 설정
with DAG(
    dag_id="kakao-pipeline", # 필수옵션, dag_id -> 구분자
    schedule_interval="@daily", # crontab 표현으로 사용가능 https://crontab.guru/
    default_args=default_args,
    tags=['kakao','api','pipeline'],
    catchup=False) as dag: # catchup=False 오늘꺼만 실행하고 과거의 작업은 실행하지않는다.

    # Operator 만들기
    creating_table = SqliteOperator(
        task_id="creating_table",
        sqlite_conn_id="db_sqlite", # UI 상에서 connection 등록
        
        # 쿼리 작성 - 테이블 만들기 (IF NOT EXISTS 넣어서 카카오 테이블이 없을때만 만들도록 설정)
        sql='''
            CREATE TABLE IF NOT EXISTS kakao_search_result(
                created_at TEXT,
                contents TEXT,
                title TEXT,
                url TEXT
            )
        '''
    )

    # HTTP 센서를 이용해서 해당 api가 존재하는지 확인
    is_api_available = HttpSensor(
        task_id="is_api_available",
        http_conn_id='kakao_api',
        endpoint="v2/search/web", # url 설정을 하는 옵션
        headers={"Authorization" : f'KakaoAK {KAKAO_API_KEY}'}, # 요청 헤더
        request_params={"query" : "아메리카노"}, # 요청 파라미터
        response_check=lambda response: response.json() # 응답 확인
    )
```



+ `response_check=lambda response: response.json()` 에서  `print(response.json())` 으로 바꿔준후  

```powershell
(lab26_env) lab26@ip-000-00-00-0:~/airflow$ airflow tasks test kakao-pipeline is_api_available 2022-01-01
```

위 코드를 터미널에서 다시 실행하면 밑에 결과처럼 작업내용들이 `print` 되서 뜬다. (근데 이방법은 비추천임)



```powershell
{'documents': [{'contents': '1. 커피[편집] 자세한 내용은 카페 <b>아메리카노</b> 문서 참고하십시오. 2. 10CM의 노래 <b>아메리카노</b>[편집] 자세한 내용은 <b>아메리카노</b>(10CM) 문서 참고하십시오. 3. 칵테일[편집] 자세한 내용은 <b>아메리카노</b>(칵테일...', 'datetime': '2021-10-04T00:00:00.000+09:00', 'title': '<b>아메리카노</b> - 나무위키', 'url': 'https://namu.wiki/w/%EC%95%84%EB%A9%94%EB%A6%AC%EC%B9%B4%EB%85%B8'}, {'contents': '1. 개요[편집] 에스프레소에 뜨거운 물을 희석시켜 만든 음료. <b>아메리카노</b>라고 줄여서 불리지만 정확한 명칭은 Caffé Americano이다. 이탈리아어인 &#39;Caffè Americano&#39;를 영역(英譯)하면 &#39;American coffee...', 'datetime': '2022-05-04T00:00:00.000+09:00', 'title': '카페 <b>아메리카노</b> - 나무위키', 'url': 'https://namu.wiki/w/%EC%B9%B4%ED%8E%98%20%EC%95%84%EB%A9%94%EB%A6%AC%EC%B9%B4%EB%85%B8'},
...
```





```python
import json

# HTTP - 실제 요청 후 응답을 받아낼수있는 Operator (데이터 획득, 추출이 가능하다!)
from airflow.providers.http.operators.http import SimpleHttpOperator

--DAG 안쪽----------------------------------------------------------------------------------------
extract_kakao = SimpleHttpOperator(
        task_id="extract_kakao",
        http_conn_id='kakao_api',
        endpoint="v2/search/web", # url 설정을 하는 옵션
        headers={"Authorization" : f'KakaoAK {KAKAO_API_KEY}'}, # 요청 헤더
        data={"query" : "아메리카노"}, # 요청 파라미터
        method="GET",  # 통신방식 설정. GET, POST 등등 설정 가능
        response_filter=lambda res : json.loads(res.text),
        log_response=True # 응답 기록들 볼껀지 선택하는 옵션
    )
```



```powershell
(lab26_env) lab26@ip-000-00-00-0:~/airflow$ airflow tasks test kakao-pipeline extract_kakao 2022-01-01
```

결과

```powershell
...
,{"contents":"안녕하세요. MSI입니다. 게이밍 모니터는 MSI에서 끝! MSI 모니터 전제품 대상으로 스타벅스 \u003cb\u003e아메리카노\u003c/b\u003e 기프티콘을 드립니다! 1. 행사기간 2022년 4월 1일 ~ 2022년 4월 30일 ​2. 경품 및 대상제품 ​2.1 경품...","datetime":"2022-04-01T17:09:00.000+09:00","title":"게이밍 모니터는 MSI에서 끝! 스타벅스 \u003cb\u003e아메리카노\u003c/b\u003e 기프티콘 100% 쏜다!","url":"http://playwares.com/sponsornews/58297211"}],"meta":{"is_end":false,"pageable_count":535,"total_count":712099}}
[2022-05-18 16:08:52,033] {taskinstance.py:1400} INFO - Marking task as SUCCESS. dag_id=kakao-pipeline, task_id=extract_kakao, execution_date=20220101T000000, start_date=, end_date=20220518T070852
```







```python
# Python Operator - Python 코드를 실행하기위한 Operator. 함수나 클래스 등을 직접 실행한다.
from airflow.operators.python import PythonOperator

--DAG 안쪽----------------------------------------------------------------------------------------
    # xcom( cross communication) 설정
    # Operator와 Operator 사이에 데이터를 전달할수있게끔 하는 도구

    preprocess_result = PythonOperator(
        task_id="preprocess_result",
        python_callable = _preprocessing # 실행시킬 함수가 들어간다.
    )
```







DAG 바깥에 만들어준다.

```python
# pip install pandas 또는 pip3 install pandas 또는 conda install pandas
# 로 터미널에서 설치해준다.
from pandas import json_normalize # json to pandas (json데이터를 pandas 데이터로 변환)


---DAG 바깥-------------------------------------------------------------------------------------
def _preprocessing(ti):
    # ti : task instance
    # dag 내의 task 의 정보를 얻어 낼수있는 객체
    search_result = ti.xcom_pull(task_ids=["extract_kakao"])

    # xcom을 이용해 가지고 온 결과가 아무것도 없다면 어떻게 처리할건지 작성
    if not len(search_result):
        raise ValueError("검색결과가 없습니다.")

    documents = search_result[0]["documents"]
    processed_documents = json_normalize([
    {"created_at": document['datetime'],
     "contents": document['contents'],
     "title": document["title"],
     "url": document["url"]} for document in documents
    ])

    processed_documents.to_csv("/tmp/processed_result.csv", index=None, header=False)

```





파이프라인 만들어주기

```python
--DAG 안쪽----------------------------------------------------------------------------------------
	# 파이프라인 구성하기
    creating_table >> is_api_available >> extract_kakao >> preprocess_result
```





csv 로 저장된 파일을 sql로 넣는 코드

```python
# bash commandline 을 입력할 수 있게 해준다.
from airflow.operators.bash import BashOperator

--DAG 안쪽----------------------------------------------------------------------------------------
# csv를 분해 후 table에 넣어주기
    store_result = BashOperator(
        task_id="store_kakao",
        bash_command='echo -e ".separator ","\n.import /home/lab26/airflow/dags/tmp/processed_result.csv kakao_search_result" | sqlite3 /home/lab26/airflow/airflow.db')

    # 파이프라인 구성하기
    creating_table >> is_api_available >> extract_kakao >> preprocess_result >> store_result
```





