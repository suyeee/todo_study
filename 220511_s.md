- 엔지니어링 과정은 스파크 위주로 갈것..

+ 컴퓨터 사양 램 16기가 이상, 노트북은 수업이 힘들수있음. 윈도우는 수업이 힘들듯.



# 데이터 엔지니어링 소개



## 데이터 엔지니어링의 목적

+ 인프라를 구성하는것.
+ 비즈니스 의사결정
  + 가격 책정
  + 모니터링
  + 분석
+ 서비스 운영/ 개선
  + A/B 테스트
  + UI/UX
  + 운영/ 자동화



## 데이터 엔지니어링의 전망

+ 데이터를 이용해서 인사이트를 추출하는 업무의 대부분은 데이터 엔지니어링이다.
+ GIGO (Garbage In Garbage Out)
+ SQL 중요
+ 인프라 구축이 사실 제일 어렵다.
+ 하둡이나 네트워크에 대한 이해가 필요
+ 어떻게 데이터를 분할해서 데이터 웨어하우스를 만들고 할것인지
+ 업무는 스파크로 진행을 할것이다.
+ 쓰레기는 만들지 말자
+ 좋은 데이터를 마련을 해서 좋은 결과를 내도록 하자



## 과거와 현재 아키텍처

+ 구축 시스템의 가격이 비싸다(과거)

  + 물리적으로 저장할 공간을 마련하는게 비쌌다.
  + 시간도 많이 들고 물적/인적 자원이 많이투입되야됨.
    + 용량 부족으로 새로운 컴퓨터 구매시 마이그레이션 작업을 하고 테스트를 해야함.

+ 데이터의 용도가 정해져있다(과거)

  + 비즈니스에 맞게 스키마를 미리 만들어 놓아야 했다.
  + 이러한 환경에서 데이터를 처리하는 방법을 ETL 이라고 부름.
    + 데이터의 형식이 거의 지정되어있고, 변동이 없는 환경에서의 데이터 파이프라인을 ETL 이라한다.
    + Extract, Transform, Load
    + 추출, 변환, 적재
    + 스키마에 맞게. 즉, 내가 정해놓은 스키마에 맞게만 적재가 진행
    + 단점: 확장성이 없다.

+ 데이터의 수집처가 일정하다(과거)

  + 유동성 확보 힘듬.

+ 요즘은 ELT 구조로 바뀌고 있다 -> 그래서 spark 를 배운다.

  

## 현재의 데이터 아키텍처

> 다양하고 형태를 예측하기 힘든



다양한 데이터의 형식

+ 실시간성을 요구하는 기능들
  + 배달어플, 택시, 지하철 등등
+ 빨라지는 기능 추가
  + 얘도 실시간성과 연관이 있다.
+ 실시간 로그
+ 비정형 데이터
+ 서드파티 데이터



그래서 스키마를 미리 전해놓고 쓰기가 힘듬.

**일단 쌓아놓고 보자!**

저렴한 컴퓨터

+ 최대한 많은 양의 데이터를 미리 저장해두고 많은 양의 프로세싱을 더 할수있게 되었다.
+ 일반적인 회사에서는 컴퓨터 파워에 대한 비용 최적화보다 비즈니스와 속도를 최적화하는 쪽이 이득이 컸다.



현재 데이터 운용 방식 (ELT)

데이터 추출 -> 일단 저장 -> 쓰임새에 따라 변환

= 스파크를 이용해서 데이터 및 로그에서 데이터를 추출 -> 스파크나 Flink 등을 



최신의 데이터 인프라 트랜드

+ 클라우드 웨어하우스인 구글 big query -> 게임회사에서 많이 쓴다함.
+ ETL -> ELT
+ Dataflow 자동화 [스케쥴러] (Airflow)
+ 누구나 데이터를 분석 (데이터 마트에 해당)



데이터 아키텍처 분야

(순서대로)

+ 소스
  + 데이터 발생
+ 수집 및 변환
  + ELT
+ 저장
  + 데이터 형식에 대한 비즈니스 적인 논의 필요
    + 정형데이터를 쓸지, 비정형데이터를 쓸지
  + 쿼리 중요함.
+ 과거 -> 예측
  + 과거는 ''배치 데이터'' 라고 함.
  + 데이터를 쌓아놓으면 그건 배치 데이터인것.
  + 
+ 출력





데이터 엔지니어링 도구들



## Batch와 Stream Processing

+ batch(배치)는 일괄 이란뜻
+ 배치는 데이터의 갯수를 셀수있다.
  + 이미 쌓여져있는 데이터니까
+ 이런 배치를 처리하는게 배치 프로세싱
  + 배치프로세싱(Batch Processing)은 일괄 처리 라는 뜻
+ **많은 양의 데이터**를 **정해진 시간**에 **한꺼번에 처리**하는것.



그렇다면 배치 프로세싱은 언제?

+ 예시..



## Stream Processing

+ **실시간으로 쏟아지는 데이터**를 **계속** 처리하는**것**
+ 과거 데이터는 now 를 통과하는순간 배치가 된다.
+ 배치 -> 유한한 데이터
+ 스트림 프로세싱 -> 무한한 미래의 데이터
  + 따라서 시간이 매우 중요
+ 이벤트가 생길때 마다, 데이터가 들어올때마다 처리
  + 왠만하면 자동화되서 관리가 되곤한다.



그렇다면 스트림 프로세싱은 언제 쓰는가?

+ 불규칙적으로 데이터가 들어오는 환경에서 사용
  + 여러개의 이벤트가 한꺼번에 들어올때
  + 오랜시간동안 이벤트가 하나도 들어오지않을때
+ 배치프로세싱의 경우
  + 주기적으로 처리
  + 배치 당 처리하는 데이터 수가 달라지면서 리소스를 비효율적으로 사용하게된다.
  + 주기적으로 처리하다보니 데이터가 안들어왔을때도 처리하는 비효율이 발생.
+ 스트림 프로세싱의 경우
  + 데이터가 생성되어 요청이 들어올떄마다 처리할수있다.
  + 데이터가 들어올때만 처리하여 효율적이다.
  + 대신에 무거운 처리는 스트림이 할수없다.
  + 무거운 처리는 배치로 처리
+ 결론
  1. 실시간성을 보장해야할때
  2. 데이터가 여러 소스로부터 들어올때 사용
  3. 데이터가 가끔 들어오거나 지속적으로 들어올때 사용
  4. 가벼운 처리를 할때 사용 (Rule-Based)



## 처리 플로우

+ 배치 처리 플로우
  1. 데이터를 모아서
  2. 데이터를 읽어서 처리 한 다음
  3. 다시 데이터베이스에 담는다.

+ 스트림 처리 플로우
  1. 데이터가 들어올때 마다(ingest)
  2. 쿼리/ 처리후 state 를 업데이트를 한 다음
  3. 저장



## 마이크로 배치

+ 배치와 스트림의 중간에 있는것.
+ 배치는 많은 양의 데이터를 처리하지만 마이크로는 데이터를 조금조금씩 모아서 처리하는 방식





# 스파크

> 대용량 분산데이터를 위해 등작했음.



## 대량데이터 처리를 위한 분산 데이터 아키텍처

+ MPP(Massively Parallel Processing)

+ HDFS



## MPP

+ 이미지 그림에 나와있는 한칸한칸이 컴퓨터임.
  + 각각 다 다른 머신임.
  + 전부 다 다른 데이터 베이스임.
+ 처리 과정
  1. 중앙 컴퓨터가 명령내림

+ 단점 : 너무 비싸다
+ 단점2 : 사실 가장 큰 문제점
  + join 등을 수행하여 집계를 하면 서로 다른 머신에 있는 데이터가 필요할수도있다.
  + 네트워크 통신이 필요
  + 근데 네트워크는? 느리다...
  + 결과물을 하나로 합쳐야됨.-> 네트워크 통신이 일어날수밖에 없음 -> 느리다 -> 만약 새로운 머신이 필요하다면 더 귀찮아짐.
  + 네트워크 계층 (interconnect) 구축을 위한 비용이 너무 많이 들었다.



## 하둡



## HDFS

+ NameNode (마스터, 대장)
+ name space/ meta data
+ NameNode -> 일 시킨다 DataNode들 한테
+ HDFS Client (따깔이)
+ DataNode (worker, slave)





## Map-Reduce

+ splitting
+ Maping
  + 내가 표현하고 싶은 데이터 형식?
+ shuffling
  + 네트워크 통신이 일어나는곳
  + 데이터를 종류별로 묶어내는
  + 모아낸 데이터를 섞어내는 작업을 하는곳 (정리)
  + 셔플링 작업을 최대한 없애서 적재하는게 엔지니어링의 역할
+ reducing
  + 합치기
  + 집계
+ 결론 :  Map-Reduce 는 과정이 복잡하고 어려워 효과적인 비즈니스를 하기는 힘들다.
+ 잘 안씀
+ 여기서 파생된게 Hive



## spark

+ 가장 큰 특징 : In-Memory 기반 수행
+ spark -> SSD 사용, hadoop -> HDD 사용
+ RDD(메모리)에서 모든작업을 하고 나중에 보낸다.
+ 이 RDD가 어디서 일어나느냐에 따라 속도 차이가 난다.
+ 



# 스파크 설치









![image-20220511131651754](220511.assets/image-20220511131651754.png)





![image-20220511131828517](220511.assets/image-20220511131828517.png)



![image-20220511131910520](220511.assets/image-20220511131910520.png)



![image-20220511132007231](220511.assets/image-20220511132007231.png)





![image-20220511132053457](220511.assets/image-20220511132053457.png)



자바와 파이스파크도 넣어준다.



![image-20220511132553068](220511.assets/image-20220511132553068.png)





path-> 편집 눌러서 환경변수편집

(PATH 를 삭제하면 컴퓨터 자체를 포맷해야할수있으므로 삭제하지않게 조심!!)

![image-20220511132648918](220511.assets/image-20220511132648918.png)





![image-20220511132749084](220511.assets/image-20220511132749084.png)



다 했으면 확인-> 확인-> 확인을 눌러준후 종료



아나콘다 프롬포트를 껏다가 다시켜서 

명령어로 PYSPARK 를 쳐서 잘 실행되는지 확인한다.



![image-20220511133100596](220511.assets/image-20220511133100596.png)



요렇게 뜨면 스파크가 잘 실행된것.





다시 아나콘다 들어가서 `SparkCourse` 라는 디렉토리를 c 드라이브에 만들어준다.

```powershell
(base) C:\WINDOWS\system32>cd \
```

```powershell
(base) C:\>mkdir SparkCourse

(base) C:\>cd SparkCourse

(base) C:\SparkCourse>
```



구글 드라이브에 올라온 `data.zip` 파일을 압축을 푼후 data 폴더를 `SparkCourse` 로 넣어준다.

![image-20220511141919340](220511.assets/image-20220511141919340.png)





주피터 노트북 실행

```powershell
(base) C:\SparkCourse>jupyter notebook
```



```python
http://localhost:4040
```

![image-20220511145853378](220511.assets/image-20220511145853378.png)

 

```python
# header = lines.first()
# header
# 오류나면서 안뜸...
```

```python
---------------------------------------------------------------------------
Py4JJavaError                             Traceback (most recent call last)
<ipython-input-26-2944ec7bcafc> in <module>
----> 1 header = lines.first()
      2 header

~\anaconda3\lib\site-packages\pyspark\rdd.py in first(self)
   1586         ValueError: RDD is empty
   1587         """
-> 1588         rs = self.take(1)
   1589         if rs:
   1590             return rs[0]

~\anaconda3\lib\site-packages\pyspark\rdd.py in take(self, num)
   1566 
   1567             p = range(partsScanned, min(partsScanned + numPartsToTry, totalParts))
-> 1568             res = self.context.runJob(self, takeUpToNumLeft, p)
   1569 
   1570             items += res
   ...
```





```python
lines.collect()
```

```python
['gender,NationalITy,PlaceofBirth,StageID,GradeID,SectionID,Topic,Semester,Relation,raisedhands,VisITedResources,AnnouncementsView,Discussion,ParentAnsweringSurvey,ParentschoolSatisfaction,StudentAbsenceDays,Class',
 'M,KW,KuwaIT,lowerlevel,G-04,A,IT,F,Father,15,16,2,20,Yes,Good,Under-7,M',
 'M,KW,KuwaIT,lowerlevel,G-04,A,IT,F,Father,20,20,3,25,Yes,Good,Under-7,M',
 'M,KW,KuwaIT,lowerlevel,G-04,A,IT,F,Father,10,7,0,30,No,Bad,Above-7,L',
 'M,KW,KuwaIT,lowerlevel,G-04,A,IT,F,Father,30,25,5,35,No,Bad,Above-7,L',
 'M,KW,KuwaIT,lowerlevel,G-04,A,IT,F,Father,40,50,12,50,No,Bad,Above-7,M',
 'F,KW,KuwaIT,lowerlevel,G-04,A,IT,F,Father,42,30,13,70,Yes,Bad,Above-7,M',
 'M,KW,KuwaIT,MiddleSchool,G-07,A,Math,F,Father,35,12,0,17,No,Bad,Above-7,L',
 'M,KW,KuwaIT,MiddleSchool,G-07,A,Math,F,Father,50,10,15,22,Yes,Good,Under-7,M',
 'F,KW,KuwaIT,MiddleSchool,G-07,A,Math,F,Father,12,21,16,50,Yes,Good,Under-7,M',
 ...
```







주피터 노트북에서 한것

```python
!pip install pyspark
```

결과

```python
DEPRECATION: Python 2.7 reached the end of its life on January 1st, 2020. Please upgrade your Python as Python 2.7 is no longer maintained. pip 21.0 will drop support for Python 2.7 in January 2021. More details about Python 2 support in pip can be found at https://pip.pypa.io/en/latest/development/release-process/#python-2-support pip 21.0 will remove support for this functionality.
Defaulting to user installation because normal site-packages is not writeable
Collecting pyspark
  Using cached pyspark-3.0.3.tar.gz (209.1 MB)
Collecting py4j==0.10.9
  Using cached py4j-0.10.9-py2.py3-none-any.whl (198 kB)
Building wheels for collected packages: pyspark
  Building wheel for pyspark (setup.py) ... done
  Created wheel for pyspark: filename=pyspark-3.0.3-py2.py3-none-any.whl size=209435970 sha256=61891494096460a46b9d63a88d40b70ffa25074e2be9cffa4775eea52c05b1da
  Stored in directory: /home/lab20/.cache/pip/wheels/7e/91/27/b83b97e172ec480ae71b04b79fbef2ce0b3dc8825ae223efe8
Successfully built pyspark
Installing collected packages: py4j, pyspark
Successfully installed py4j-0.10.9 pyspark-3.0.3
```



```python
from pyspark import SparkConf, SparkContext
```



```python
conf = SparkConf().setMaster('local').setAppName('country-student-counts')
sc = SparkContext(conf = conf)
```

결과

```python
/home/lab20/.local/lib/python3.6/site-packages/pyspark/context.py:238: FutureWarning: Python 3.6 support is deprecated in Spark 3.2.
  FutureWarning
```



```python
!pwd
```

```python
/home/lab20
```



```python
directory = '/home/lab20/data'
filename = 'xAPI-Edu-Data.csv'
```





```python
lines =  sc.textFile(f"file:///{directory}:{filename}")
lines
```

```python
file:////home/lab20/data:xAPI-Edu-Data.csv MapPartitionsRDD[1] at textFile at NativeMethodAccessorImpl.java:0
```



```python
header = lines.first()
header
```

```python
'gender,NationalITy,PlaceofBirth,StageID,GradeID,SectionID,Topic,Semester,Relation,raisedhands,VisITedResources,AnnouncementsView,Discussion,ParentAnsweringSurvey,ParentschoolSatisfaction,StudentAbsenceDays,Class'
```





```python
sc.stop()
```



