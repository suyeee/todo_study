# 실습과제 답 확인

[과제2](220513_s.assets/05-12 과제2.ipynb)



# 스파크 클러스터의 구조

![image-20220513100517111](220513.assets/image-20220513100517111.png)

+ 모든 통제는 마스터가 한다.
+ 워커들 -> 각종 연산들 수행





![image-20220513101656401](220513.assets/image-20220513101656401.png)



이때 Task 는 실제 프로그램을 전송하는거라고 보면된다.



워커노드에서 작업된 결과물을 디스크에 저장.



![image-20220513101630696](220513.assets/image-20220513101630696.png)



# Transformations

- 새로운 RDD를 만들어 나가는 과정



## Action의 종류

- 대부분의 Action은 Reduction 이다
-  Reduction 이란?
  - **근접하는 요소**들을 모아서 **하나의 결과로 만드는 일** -> 그냥 Group By 로 보면된다.
  - 그럼 어떻게 해야 Group By가 잘되는가? (좀더 빠르게, 잘되게 하는법)
  - 관련성 있는 데이터들을 어떻게 묶어주면 좋은가?



## 병렬처리가 가능한 Reduction

![image-20220513102106557](220513.assets/image-20220513102106557.png)

+ Task 끼리 서로 독립적이다.
+ 다른 Task에 의존하지않고 자체적으로 연산이 가능



## 병렬처리가 불가능한 Reduction

![image-20220513102236590](220513.assets/image-20220513102236590.png)

![image-20220513102326560](220513.assets/image-20220513102326560.png)



+ 만약 C=A+B 라는 Task 라면
+ A Task가 끝날때까지 다른 애들은 놀고있어야됨.
+ 이런 경우엔 분산처리를 (쓰레드를) 안쓰는게 오히려 낫다.
+ 분산 병렬처리가 안되는 케이스



# Key-Value RDD의 Operations & Joins


## K-V RDD의 Operations

![image-20220513102654070](220513.assets/image-20220513102654070.png)



+ 파티션이 유지가 안되더라도 

![image-20220513103015129](220513.assets/image-20220513103015129.png)





# Shuffling

![image-20220513103242142](220513.assets/image-20220513103242142.png)



+ 데이터를 **<u>그룹화</u>** 할때 데이터를 **한 노드에서 다른 노드로 <u>옮길때</u>** 발생한다.
+ 파티션을 넘나들게되면 셔플링이 일어난다.
+ 네트워크통신으로 옮길때 성능저하가 심하게 일어난다.
  + 같은 노드내에서 옮기는거보다



![image-20220513103450881](220513.assets/image-20220513103450881.png)

+ 여러번의 셔플링이 발생할수밖에 없다

+ 셔플링은 적은게 좋다.





![image-20220513104200523](220513.assets/image-20220513104200523.png)



![image-20220513104643933](220513.assets/image-20220513104643933.png)

+ 비슷한 데이터는 비슷한 데이터끼리 모이게끔 해주는 함수



## 실습

![image-20220513110359089](220513.assets/image-20220513110359089.png)

![image-20220513110504413](220513.assets/image-20220513110504413.png)

![image-20220513110556210](220513.assets/image-20220513110556210.png)

![image-20220513110831223](220513.assets/image-20220513110831223.png)

![image-20220513110939174](220513.assets/image-20220513110939174.png)

위의 4번 셀과 같은 코드나 마찬가지 (5번셀이)



6번셀 설명

![image-20220513111145362](220513.assets/image-20220513111145362.png)

![image-20220513111335840](220513.assets/image-20220513111335840.png)

7번 셀 설명

![image-20220513111732364](220513.assets/image-20220513111732364.png)

![image-20220513112040432](220513.assets/image-20220513112040432.png)



8번셀 설명

![image-20220513111854330](220513.assets/image-20220513111854330.png)



![image-20220513111828257](220513.assets/image-20220513111828257.png)

![image-20220513112014652](220513.assets/image-20220513112014652.png)

![image-20220513112126833](220513.assets/image-20220513112126833.png)

![image-20220513112200125](220513.assets/image-20220513112200125.png)

![image-20220513112228212](220513.assets/image-20220513112228212.png)

![image-20220513112304964](220513.assets/image-20220513112304964.png)

짝수끼리 모임.

![image-20220513112408838](220513.assets/image-20220513112408838.png)

![image-20220513112542175](220513.assets/image-20220513112542175.png)

![image-20220513112427385](220513.assets/image-20220513112427385.png)

![image-20220513112700615](220513.assets/image-20220513112700615.png)





그리고 나서 예제 6번 `06. Key-Value RDD Operations, Joins` 보면됨.



`reduceByKey` = `groupByKey` + `reduce`

그룹핑을 먼저해주면 셔플링이 많이 발생함. 따라서 순서는 reduce를 먼저해주고 그다음 그룹핑을 해주면 셔플링을 최소화할수있다.



countByKey -> 요소를 감싼 리스트의 갯수를 반환

따라서 하의 1개, 상의 1개로 나온것.



Keys()

키의 갯수는 데이터의 갯수와 같다.

키의 종류를 보고 싶다면 `keys().distinct().count()`로 해주면 된다.



joins

- `Inner Join` : 서로간에 존재하는 키만 합쳐준다.
- `Outer Join` : 기준이 되는 한쪽에는 데이터가 있고, 다른 쪽에는 데이터가 없는 경우
  - 설정한 기준에 따라 기준에 맞는 데이터가 항상 남아있는다.
  - `leftOuterJoin` : 왼쪽에 있는 RDD 가 기준이 된다. (함수를 호출하는 쪽)
  - `rightOuterJoin` : 오른쪽에 있는 RDD 가 기준이 된다. (함수에 매개변수로 들어가는 쪽)



```python
rdd1 = sc.parallelize([
    ("foo", 1),
    ("goo", 2),
    ("hoo", 3)
])

rdd2 = sc.parallelize([
    ("foo", 1),
    ("goo", 2),
    ("goo", 10),
    ("moo", 6)
])
```



```python
# Inner Join
rdd1.join(rdd2)
```





# 쳅터4 : Spark SQL, DataFrame, Dataset

![image-20220513132427855](220513.assets/image-20220513132427855.png)



## 데이터 (구조 VS 비구조)

![image-20220513133036515](220513.assets/image-20220513133036515.png)

(그림 참고해서 설명적고 그림 지우기)

+ 비구조 데이터

  + 형식이 없는 데이터
    + 로그파일
    + 이미지
  + 데이터 레이크, 데이터 웨어하우스에 해당

+ 준구조데이터

  + 행과 열을 가짐
    + CSV
    + JSON
    + XML

  + 데이터 레이크, 데이터 웨어하우스에 해당

+ 구조 데이터
  + 행과 열, 데이터 타입(스키마)도 가짐
    + 데이터베이스
  + 데이터 마트에 해당

+ 구조 데이터를 다루는게 더 쉽다.





## 데이터를 합치고 추출하는 법

1. Join을 해야한다.
2. 관객수 500만 이상인 영화만 추출하려면?
   1. Join -> filter
   2. filter -> Join : 이 방법이 더 괜찮은 방법! (셔플링 떄문에)

+ 매번 이렇게 순서에 대한 고민을 한다면?
  + 개발자 마다 성능차이가 많이 나게된다.
  + 개발자의 편차가 너무 심해진다.
+ 근데 과연 데이터가 구조화된 데이터 라면?
+ 구조화딘 데이터는 자동으로 최적화가 가능하다!!!
  + 여기서 말하는 최적화란 알아서 파티션도 고려해주고 셔플링도 고려해주며 알아서 괜찮은 방법으로 해주는것.



## RDD와 구조화된 데이터의 차이

+ RDD에서는	

  + 사용자가 만든 `function(=Task)` 을 수행한다.

  

+ 스파크

  + 어떤 Task를 할껀지만 결정지어주면된다.
  + 데이터를 구조화된 데이터로만 만들어주면 나머지는 다 알아서 스파크가 처리해준다.



## Spark SQL를 이용해 구조화된 데이터 다루기

+ Spark SQL은 구조화된 데이터를 다룰수있게해준다.
  + 사용자가 일일히 Function을 



# Spark SQL의 목적

+ 스파크 프로그래밍 내부에서 <u>관계형 처리</u>를 하기위함 
  + 관계형 처리 -> Join
+ <u>스키마의 정보</u>를 이용해 자동으로 최적화를 하기위함
+ <u>외부데이터 세트</u>를 사용하기 쉽게 하기위함



## Spark SQL

+ 스파크를 기반으로 구현된 하나의 패키지
+ 카탈리스트 -> 쿼리를 최적화시켜주는 친구
+ 물리적인 최적화 -> 텅스텐
+ 논리적인 최적화 -> 카탈리스트



# DataFrame

+ SQL과 DataFrame 둘다 RDD 기반임.
+ 판다스의 데이터프레임과 굉장히 흡사함.
+ RDD에 스키마가 적용된것
+ 더이상 스파크context 안쓴다



## DataFrame 만들기

![image-20220513134559480](220513.assets/image-20220513134559480.png)



+ 전에 RDD 실습할때는 항상 header를 가져와서 날려버렸는데 이제는 header가 필요함.



## 💡createOrReplaceTempView (중요)





# Datasets







# 노트 필기

![image-20220513140516735](220513.assets/image-20220513140516735.png)



![image-20220513140835883](220513.assets/image-20220513140835883.png)



![image-20220513141035075](220513.assets/image-20220513141035075.png)

![image-20220513141235726](220513.assets/image-20220513141235726.png)



+ 데이터 프레임과 SQL은 거의 똑같다고 보면된다.
+ 대신 스키마를 우리가 직접 지정가능
+ 컬럼의 이름부여 가능



![image-20220513141332874](220513.assets/image-20220513141332874.png)



+ RDD를 발전시켜서 SQL과 DF (데이터프레임)을 만들었다.



# Spark DataFrame, SQL 실습































